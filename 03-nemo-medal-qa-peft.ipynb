{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1f74ada-05b3-43da-b6de-c7e12745c66a",
   "metadata": {},
   "source": [
    "<center>\n",
    "  <a href=\"https://escience.sdu.dk/index.php/ucloud/\">\n",
    "    <img src=\"https://escience.sdu.dk/wp-content/uploads/2020/03/logo_esc.svg\" width=\"400\" height=\"186\" />\n",
    "  </a>\n",
    "</center>\n",
    "<br>\n",
    "<p style=\"font-size: 1.2em;\">\n",
    "  This notebook was tested on UCloud using <strong>NeMo Framework v25.02.01</strong> and machine types: <code>u2-gpu-4</code> (NVIDIA A100) and <code>u3-gpu-4</code> (NVIDIA H100).\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3323204-1463-4df3-8c75-5e95b6d66ba1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 03 - Llama 3.1 Fine-tuning: Training on a Synthetic Medical Q&A Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a06833b-ba38-4deb-81d7-4fea58cd638f",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, our goal is to **fine-tune the Llama 3.1 Instruct model** for a medical question-answering task.\n",
    "\n",
    "We will:\n",
    "- **Use a synthetic medical Q&A dataset** generated in [`02-ls-create-medal-qa-dataset.ipynb`](02-ls-create-medal-qa-dataset.ipynb).\n",
    "\n",
    "- **Apply LoRA (Low-Rank Adaptation)** for parameter-efficient fine-tuning.\n",
    "\n",
    "- **Demonstrate the full workflow**, including loading the dataset and model, configuring the training, and evaluating the fine-tuned model's Q&A performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3295c949-d65f-4f1c-8439-96b707c2b3a4",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è **Step 1: Environment Check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d91fd8-4f6a-4e11-8224-f707f3e47b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.get_device_name()\n",
    "    print(f\"‚úÖ GPU detected: {device}\")\n",
    "else:\n",
    "    raise RuntimeError(\"‚ùå No GPU detected! Ensure your UCloud session uses a GPU node.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ec4687-f5c2-4004-abc4-0508842dd092",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è **Step 2: Download and Convert Pre-trained Model from Hugging Face**\n",
    "\n",
    "We use Hugging Face's `transformers` library and NeMo conversion utilities to fetch an open-source checkpoint and convert it into NeMo `.nemo` format.\n",
    "\n",
    "**Why convert?**\n",
    "- NeMo's inference and training pipelines expect models in `.nemo` format, which bundles both the model weights and configuration in a single file.\n",
    "- Ensures compatibility with NeMo's `restore_from` and `save_to` methods for seamless loading.\n",
    "\n",
    "**Hugging Face repo:**  [Llama 3.1 8B Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2435f1d6-a508-4423-b11e-a2a369dbf12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import Password\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "pwd = Password(description=\"Hugging Face Token:\")\n",
    "display(pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dcddea-2bd1-41ca-be05-e9f9bb7a1dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = pwd.value\n",
    "hf_model=\"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "hf_model_path=\"models/llama-3.1-instruct/8B/hf\"\n",
    "snapshot_download(\n",
    "    repo_id=hf_model,\n",
    "    local_dir=hf_model_path,\n",
    "    token=token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceaaddf-3f5d-4558-9336-d1f43d647356",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$hf_model_path\"\n",
    "\n",
    "ls $1\n",
    "du -sh $1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a37b884-0ac1-43a3-81d9-d65e89d7f95b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Convert the Model in NeMo Format\n",
    "\n",
    "HF_MODEL=\"models/llama-3.1-instruct/8B/hf\"\n",
    "PRECISION=bf16\n",
    "NEMO_MODEL=\"models/llama-3.1-instruct/8B/nemo/$PRECISION/Llama-3_1-Instruct-8B.nemo\"\n",
    "\n",
    "export TOKENIZERS_PARALLELISM=true\n",
    "export NUMEXPR_MAX_THREADS=$(nproc)\n",
    "\n",
    "# Convert model to .nemo \n",
    "python3 -W ignore /opt/NeMo/scripts/checkpoint_converters/convert_llama_hf_to_nemo.py \\\n",
    "        --input_name_or_path \"$HF_MODEL\" \\\n",
    "        --output_path \"$NEMO_MODEL\" \\\n",
    "        --precision \"$PRECISION\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b285d5a-d838-423b-9d6c-65add61f48ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "## üõ†Ô∏è **Step 3: Prepare the Dataset (Medal-QA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b90e3dd-f002-4337-836b-269176cc77d0",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df9602d-d7f3-4118-9504-30a139c57794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read all data into a list of dictionaries\n",
    "dataset = []\n",
    "with open(\"datasets/medal-qa_synthetic_dataset_v1.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        dataset.append(json.loads(line))\n",
    "\n",
    "# Preview the first 3 examples\n",
    "for idx, item in enumerate(dataset[:3]):\n",
    "    print(f\"Example {idx+1}:\\n{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bdc3d4-2335-47ac-955a-7f30cae6d88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29e9e38-f688-4e1b-b368-569769cfeb7e",
   "metadata": {},
   "source": [
    "### Perform train/validation/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949fa246-1f28-45c2-adbf-f2b20fe0455b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('json', data_files='datasets/medal-qa_synthetic_dataset_v1.jsonl', split='train')\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "# Only keep \"question\" and \"answer\" fields\n",
    "dataset = dataset.remove_columns([col for col in dataset.column_names if col not in [\"question\", \"answer\"]])\n",
    "\n",
    "# Step 1: Split into 80% train and 20% temp (temp will be further split into validation and test)\n",
    "train_temp = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# Step 2: Split temp into 50% validation and 50% test (each gets 10% of total data)\n",
    "val_test = train_temp[\"test\"].train_test_split(test_size=0.5)\n",
    "\n",
    "# Final splits\n",
    "train_dataset = train_temp[\"train\"]\n",
    "validation_dataset = val_test[\"train\"]\n",
    "test_dataset = val_test[\"test\"]\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Train size: {len(train_dataset)}, Validation size: {len(validation_dataset)}, Test size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122f903a-492c-4ef9-a72b-3763f552fcad",
   "metadata": {},
   "source": [
    "### Convert to NeMo JSONL format\n",
    "\n",
    "We leverage the `save_jsonl_preprocessed` utility function to prefix each example's `input` with a detailed instruction.\n",
    "This instruction sets the model's role and response style before presenting the actual question, improving task understanding and guiding generation.\n",
    "\n",
    "Each JSONL record will follow NeMo's expected `{input} {output}` prompt format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "518b5804-0175-4718-8237-0659c7f0252b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, json\n",
    "from typing import List, Literal, Optional\n",
    "\n",
    "from datasets import DatasetDict, concatenate_datasets, load_dataset, load_from_disk\n",
    "from datasets.builder import DatasetGenerationError\n",
    "\n",
    "# Customized conversion with instruction prefix\n",
    "def save_jsonl_preprocessed(\n",
    "    dataset,\n",
    "    filename,\n",
    "    instruction=(\n",
    "        \"You are a board-certified medical professional and \"\n",
    "        \"a skilled communicator. Provide accurate, evidence‚Äëbased answers \"\n",
    "        \"to medical questions in clear, concise language, suitable for both \"\n",
    "        \"healthcare providers and patients.\"\n",
    "    ),\n",
    "):\n",
    "    \"\"\"\n",
    "    Writes a JSONL file where each line is:\n",
    "      {\"input\": \"<instruction> Question: ‚Ä¶\\n\\n### Response:\\n\", \"output\": \"‚Ä¶\"}\n",
    "    with *actual* newlines and one JSON record per line.\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        for example in dataset:\n",
    "            q = example[\"question\"]\n",
    "            a = example[\"answer\"]\n",
    "            # Use real newlines (\\n), not literal backslashes\n",
    "            inp = f\"{instruction} Question: {q}\\n\\n### Response:\\n\"\n",
    "            json.dump({\"input\": inp, \"output\": a}, f, ensure_ascii=False)\n",
    "            f.write(\"\\n\") \n",
    "\n",
    "# after your splits:\n",
    "save_jsonl_preprocessed(train_dataset,      \"datasets/medal_train.jsonl\")\n",
    "save_jsonl_preprocessed(validation_dataset, \"datasets/medal_validation.jsonl\")\n",
    "save_jsonl_preprocessed(test_dataset,       \"datasets/medal_test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3357b3c7-dbc1-4086-8c32-090a739fcb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n3 datasets/medal_train.jsonl | jq ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f30a6b-ac58-4095-a333-ef02a9138076",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n3 datasets/medal_validation.jsonl | jq ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453bfc7d-fafb-4f4c-9576-e12a0674ebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n3 datasets/medal_test.jsonl | jq ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb1d887",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è **Step 4: LoRA Fine-Tuning with NeMo's High-Level Script**\n",
    "\n",
    "In this step, we‚Äôll set up and launch the LoRA fine-tuning run using NeMo‚Äôs high-level model fine-tuning script. We only need to specify a few essential parameters‚Äîdataset paths, PEFT scheme, optimizer settings, parallelism degrees, and batch sizes‚Äîto get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faff7580-e646-4b30-bf87-94818e9b1f68",
   "metadata": {},
   "source": [
    "### ü§î Understanding Fine-Tuning Objectives for LLMs\n",
    "\n",
    "Fine-tuning a large language model (LLM) like Llama 3.1 does not mean memorizing every possible answer verbatim‚Äîit means:\n",
    "\n",
    "1. **Adapting to domain-specific language and style**: The model learns terminology, phrasing, and response formats relevant to medical Q&A.\n",
    "2. **Improving factual consistency**: By training on medical question‚Äìanswer pairs, the model reinforces evidence-based associations.\n",
    "3. **Enhancing reasoning patterns**: Exposure to step-by-step medical explanations helps the model generalize reasoning to new questions.\n",
    "\n",
    "Since test questions may never have appeared in training, we do not expect exact matches. Instead, we measure:\n",
    "- **Content relevance**: Does the generated answer address the question accurately?\n",
    "- **Factual correctness**: Are medical facts presented correctly, even if worded differently?\n",
    "- **Clarity and completeness**: Is the response concise yet informative, following the instruction prompt?\n",
    "\n",
    "In summary, fine-tuning refines the LLM‚Äôs ability to **generalize** medical Q&A skills to unseen queries, not to retrieve memorized answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e236be8-ee7f-4d76-ad6e-5572e12bdd0e",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Data Parallelism & Gradient Accumulation\n",
    "\n",
    "In large-scale training, we often want to process more samples than a single GPU can handle at once. Two techniques help:\n",
    "\n",
    "1. **Data Parallelism:** Each GPU holds a copy of the model and processes different micro‚Äëbatches in parallel. After each backward pass, gradients are summed across GPUs.\n",
    "2. **Gradient Accumulation:** Instead of updating weights after every micro‚Äëbatch, we accumulate gradients over multiple micro‚Äëbatches to simulate a larger **global batch** while keeping memory usage constant.\n",
    "\n",
    "#### Visual Illustration of Accumulation\n",
    "\n",
    "```text\n",
    "‚îå‚îÄ micro‚Äëbatch #1 ‚îÄ‚î¨‚îÄ micro‚Äëbatch #2 ‚îÄ‚î¨‚îÄ micro‚Äëbatch #3 ‚îÄ‚î¨‚îÄ micro‚Äëbatch #4 ‚îÄ‚îê\n",
    "‚îÇ                  ‚îÇ                  ‚îÇ                  ‚îÇ                  ‚îÇ\n",
    "‚îÇ   forward pass   ‚îÇ   forward pass   ‚îÇ   forward pass   ‚îÇ   forward pass   ‚îÇ\n",
    "‚îÇ     (loss‚ÇÅ)      ‚îÇ     (loss‚ÇÇ)      ‚îÇ     (loss‚ÇÉ)      ‚îÇ     (loss‚ÇÑ)      ‚îÇ\n",
    "‚îÇ        ‚Üì         ‚îÇ        ‚Üì         ‚îÇ        ‚Üì         ‚îÇ        ‚Üì         ‚îÇ\n",
    "‚îÇ  backward pass   ‚îÇ  backward pass   ‚îÇ  backward pass   ‚îÇ  backward pass   ‚îÇ\n",
    "‚îÇ (‚àÇl‚ÇÅ/‚àÇŒ∏)  +=     ‚îÇ (‚àÇl‚ÇÇ/‚àÇŒ∏)  +=     ‚îÇ (‚àÇl‚ÇÉ/‚àÇŒ∏)  +=     ‚îÇ (‚àÇl‚ÇÑ/‚àÇŒ∏)  =      ‚îÇ\n",
    "‚îÇ accumulate grads ‚îÇ accumulate grads ‚îÇ accumulate grads ‚îÇ accumulate grads ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                                            ‚îÇ\n",
    "                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                        ‚îÇ            optimizer step            ‚îÇ\n",
    "                        ‚îÇ      Œ∏ ‚Üê Œ∏ ‚àí lr ¬∑ Œ£¬†grads¬†/¬†g_batch  ‚îÇ\n",
    "                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "- **Micro batch:** Number of samples processed per GPU per forward/backward pass (e.g., 16).\n",
    "- **Gradient accumulation steps:** Number of consecutive micro‚Äëbatches (forward/backward passes) each GPU processes‚Äîaccumulating (i.e. summing) their gradients‚Äîbefore performing a single optimizer update. \n",
    "- **Global batch:** Total samples whose gradients contribute to one weight update across all GPUs; equals: `micro-batch¬†size √ó gradient¬†accumulation¬†steps √ó number¬†of¬†GPUs`.\n",
    "\n",
    "#### Example of Gradient Accumulation\n",
    "\n",
    "```text\n",
    "# Settings:\n",
    "micro_batch_size       = 16    # samples per GPU per forward/backward pass\n",
    "gradient_accumulation  = 1     # micro‚Äëbatches per weight update on each GPU\n",
    "num_GPUs               = 4\n",
    "global_batch_size      = micro_batch_size √ó gradient_accumulation √ó num_GPUs\n",
    "                       = 16 √ó 1 √ó 4\n",
    "                       = 64    # samples per weight update\n",
    "\n",
    "# Then each global_step processes one global batch (64 samples):\n",
    "global_step = 1   ‚Üí processed 1 √ó 64   =   64 samples ‚Üí 1st weight update  \n",
    "global_step = 2   ‚Üí processed 2 √ó 64   =  128 samples ‚Üí 2nd weight update  \n",
    "‚Ä¶  \n",
    "global_step = 426 ‚Üí processed 426 √ó 64 = 27264 samples ‚Üí 426th weight update (‚âà1¬†epoch) for a training dataset of 27000 lines\n",
    "```\n",
    "\n",
    "#### Detailed Workflow \n",
    "| Stage                 | Description                                                                                                                                                                                   | Purpose                                                                                                  |\n",
    "|-----------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|\n",
    "| Forward pass          | The network runs on input data, produces logits, and computes a scalar loss.                                                                                                                   | Needed to know how wrong the current weights are.                                                           |\n",
    "| Backward pass         | PyTorch¬†autograd walks the graph in reverse, computing gradients (‚àÇloss¬†/¬†‚àÇŒ∏) for every parameter¬†Œ∏.                                                                                            | Gives the direction to adjust each weight.                                                                  |\n",
    "| Gradient accumulation | Instead of calling `optimizer.step()` immediately, we add these gradients to a running buffer.                                                                                                 | Lets us mimic a larger batch without fitting all samples in memory.                                         |\n",
    "| Optimizer step        | After we have accumulated gradients from enough micro‚Äëbatches to equal the global batch size, we update the weights once (SGD,¬†Adam,¬†etc.), then zero the grad buffers.                          | This is the true training step seen by the learning‚Äërate scheduler and appears in training logs/metrics.    |\n",
    "\n",
    "This approach provides flexibility:\n",
    "- **Larger effective batch sizes** for stable training and better convergence.\n",
    "- **Memory efficiency** by keeping per-step memory constant.\n",
    "- **Scalability** across multiple GPUs with straightforward gradient synchronization.\n",
    "‚ÄØ264 samples ‚Üí 426th weight update (‚âà1‚ÄØepoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc6f91a-d2b7-45f4-add5-73014b1e31c0",
   "metadata": {},
   "source": [
    "### üöÄ Launching the LoRA Fine-Tuning Script\n",
    "\n",
    "NeMo framework includes a high level Python script for fine-tuning, [megatron_gpt_finetuning.py](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py), that can abstract away some of the lower level API calls. Once you have your model downloaded and the dataset ready, LoRA fine-tuning with NeMo is essentially just running this script!\n",
    "\n",
    "Some of the relevant settings are:\n",
    "\n",
    "#### Training dataset JSONL file(s)\n",
    "```bash\n",
    "model.data.train_ds.file_names='datasets/medal_train.jsonl'\n",
    "```\n",
    "#### Validation dataset JSONL file(s)\n",
    "```bash\n",
    "model.data.validation_ds.file_names='datasets/medal_validation.jsonl'\n",
    "```\n",
    "#### PEFT method: LoRA scheme\n",
    "```bash\n",
    "model.peft.peft_scheme=lora\n",
    "```\n",
    "#### O2-level automatic mixed precision\n",
    "```bash\n",
    "model.megatron_amp_O2=True\n",
    "```\n",
    "#### Optimizer and learning rate configuration\n",
    "```bash\n",
    "model.optim.name=fused_adam\n",
    "model.optim.lr=5e-6\n",
    "```\n",
    "#### Tensor model parallelism across model layers\n",
    "```bash\n",
    "model.tensor_model_parallel_size=1\n",
    "```\n",
    "#### Pipeline model parallelism across model stages\n",
    "```bash\n",
    "model.pipeline_model_parallel_size=1  \n",
    "```\n",
    "#### Effective batch size across all GPUs and gradient accumulation steps\n",
    "```bash\n",
    "model.global_batch_size=64 \n",
    "```\n",
    "#### Number of samples per GPU per forward/backward pass\n",
    "```bash\n",
    "model.micro_batch_size=16\n",
    "```\n",
    "For this demonstration, this training run is capped by `max_steps`, and validation is carried out every `val_check_interval` steps. If the validation loss does not improve after a few checks, training is halted to avoid overfitting.\n",
    "\n",
    "> `NOTE:` In the block of code below, pass the paths to your train and validation data files as well as path to the `.nemo` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899a230d-51b2-467e-9ea2-aa59a23f08e6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash -s \"$token\"\n",
    "\n",
    "# Log in to HuggingFace to get AutoTokenizer with pretrained_model_name\n",
    "HF_TOKEN=\"$1\"\n",
    "huggingface-cli login --token \"$HF_TOKEN\"\n",
    "\n",
    "# Set paths to the model, train, validation and test sets.\n",
    "PRECISION=bf16\n",
    "MODEL=\"models/llama-3.1-instruct/8B/nemo/$PRECISION/Llama-3_1-Instruct-8B.nemo\"\n",
    "\n",
    "OUTPUT_DIR=\"lora/llama-3.1-instruct-medal/8B/$PRECISION\"\n",
    "rm -rf \"$OUTPUT_DIR\"\n",
    "\n",
    "TRAIN_DS=\"['datasets/medal_train.jsonl']\"\n",
    "VALID_DS=\"['datasets/medal_validation.jsonl']\"\n",
    "\n",
    "SCHEME=\"lora\"\n",
    "GPUS=4 # Adjust if necessary\n",
    "TP_SIZE=1\n",
    "PP_SIZE=1\n",
    "\n",
    "# Monitor training using WandB\n",
    "export WANDB_API_KEY=\"your_wandb_api_key\"\n",
    "WANDB_LOGGER=False # Set equal to True to instantiate a¬†WandB logger\n",
    "WANDB_PROJECT=\"Medal-QA\"\n",
    "\n",
    "export PYTHONWARNINGS=\"ignore\"\n",
    "\n",
    "torchrun --nproc_per_node=${GPUS} \\\n",
    "/opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py \\\n",
    "    exp_manager.exp_dir=${OUTPUT_DIR} \\\n",
    "    exp_manager.explicit_log_dir=${OUTPUT_DIR} \\\n",
    "    trainer.devices=${GPUS} \\\n",
    "    trainer.num_nodes=1 \\\n",
    "    trainer.precision=${PRECISION} \\\n",
    "    trainer.val_check_interval=4000 \\\n",
    "    trainer.max_steps=24000 \\\n",
    "    exp_manager.early_stopping_callback_params.patience=3 \\\n",
    "    model.megatron_amp_O2=True \\\n",
    "    ++model.mcore_gpt=True \\\n",
    "    ++model.dist_ckpt_load_strictness=log_all \\\n",
    "    model.tensor_model_parallel_size=${TP_SIZE} \\\n",
    "    model.pipeline_model_parallel_size=${PP_SIZE} \\\n",
    "    model.global_batch_size=16 \\\n",
    "    model.micro_batch_size=4 \\\n",
    "    model.restore_from_path=${MODEL} \\\n",
    "    model.data.train_ds.file_names=${TRAIN_DS} \\\n",
    "    model.data.train_ds.concat_sampling_probabilities=[1.0] \\\n",
    "    model.data.validation_ds.file_names=${VALID_DS} \\\n",
    "    model.peft.peft_scheme=${SCHEME} \\\n",
    "    model.optim.name=fused_adam \\\n",
    "    model.optim.lr=5e-6 \\\n",
    "    exp_manager.create_wandb_logger=${WANDB_LOGGER} \\\n",
    "    exp_manager.wandb_logger_kwargs.project=${WANDB_PROJECT} \\\n",
    "    exp_manager.resume_if_exists=True \\\n",
    "    exp_manager.create_checkpoint_callback=True \\\n",
    "    exp_manager.checkpoint_callback_params.monitor=validation_loss \\\n",
    "    exp_manager.resume_ignore_no_checkpoint=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe41ef6-7514-4c5d-bce9-ae1daecce770",
   "metadata": {},
   "source": [
    "This will create a LoRA adapter - a file named `megatron_gpt_peft_lora_tuning.nemo` in `./lora/llama-3.1-instruct-medal/.../checkpoints/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cc422f-f654-4694-8832-b7455f397b7f",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è **Step 5: Model Evaluation with NeMo's Generation Script**\n",
    "\n",
    "After fine-tuning and saving LoRA adapters, we evaluate the model by generating answers on the test set using NeMo's high-level generation script:\n",
    "[megatron_gpt_generate.py](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py).\n",
    "\n",
    "We'll compute two metrics:\n",
    "- **Exact Match (EM)**: whether the prediction exactly matches the label.\n",
    "- **Token-level F1**: overlap between prediction and label tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549a6537-f0f3-40dd-ab52-a423a85f0371",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check that the LORA model file exists\n",
    "\n",
    "python -c \"import torch; torch.cuda.empty_cache()\"\n",
    "\n",
    "PRECISION=bf16\n",
    "OUTPUT_DIR=\"lora/llama-3.1-instruct-medal/8B/$PRECISION\"\n",
    "ls -l $OUTPUT_DIR/checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a2a2bb-f5ad-4077-a516-1ac6c7ad1f7c",
   "metadata": {},
   "source": [
    "In the code snippet below, the following configurations are worth noting: \n",
    "\n",
    "1. `model.restore_from_path` to the path for the `Llama-3_1-Instruct-8B.nemo` file.\n",
    "2. `model.peft.restore_from_path` to the path for the PEFT checkpoint that was created in the fine-tuning run in the last step.\n",
    "3. `model.test_ds.file_names` to the path of the `medal_test.jsonl` file.\n",
    "\n",
    "If you have made any changes in model or experiment paths, please ensure they are configured correctly below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93108124-32a5-4c8f-ab25-52dbe9b26ac6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash -s \"$token\"\n",
    "\n",
    "# Log in to HuggingFace to get AutoTokenizer with pretrained_model_name\n",
    "HF_TOKEN=\"$1\"\n",
    "huggingface-cli login --token \"$HF_TOKEN\"\n",
    "\n",
    "PRECISION=bf16\n",
    "MODEL=\"models/llama-3.1-instruct/8B/nemo/$PRECISION/Llama-3_1-Instruct-8B.nemo\"\n",
    "OUTPUT_DIR=\"lora/llama-3.1-instruct-medal/8B/$PRECISION\"\n",
    "TEST_DS=\"[datasets/medal_test.jsonl]\"\n",
    "TEST_NAMES=\"[medal]\"\n",
    "SCHEME=\"lora\"\n",
    "GPUS=4 # Adjust if necessary\n",
    "TP_SIZE=1\n",
    "PP_SIZE=1\n",
    "\n",
    "# This is where your LoRA checkpoint was saved\n",
    "PATH_TO_TRAINED_MODEL=\"$OUTPUT_DIR/checkpoints/megatron_gpt_peft_lora_tuning.nemo\"\n",
    "\n",
    "# The generation run will save the generated outputs over the test dataset in a file prefixed like so\n",
    "OUTPUT_PREFIX=\"results/medalQA_result_lora_tuning_\"\n",
    "\n",
    "export PYTHONWARNINGS=\"ignore\"\n",
    "export TOKENIZERS_PARALLELISM=true\n",
    "\n",
    "torchrun --nproc_per_node=${GPUS} \\\n",
    "/opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py \\\n",
    "    model.restore_from_path=${MODEL} \\\n",
    "    model.peft.restore_from_path=${PATH_TO_TRAINED_MODEL} \\\n",
    "    trainer.devices=${GPUS} \\\n",
    "    trainer.num_nodes=1 \\\n",
    "    trainer.precision=${PRECISION} \\\n",
    "    model.megatron_amp_O2=True \\\n",
    "    model.global_batch_size=16 \\\n",
    "    model.micro_batch_size=4 \\\n",
    "    model.data.test_ds.file_names=${TEST_DS} \\\n",
    "    model.data.test_ds.names=${TEST_NAMES} \\\n",
    "    model.data.test_ds.global_batch_size=16 \\\n",
    "    model.data.test_ds.tokens_to_generate=128 \\\n",
    "    model.tensor_model_parallel_size=${TP_SIZE} \\\n",
    "    model.pipeline_model_parallel_size=${PP_SIZE} \\\n",
    "    inference.greedy=True \\\n",
    "    model.data.test_ds.output_file_path_prefix=${OUTPUT_PREFIX} \\\n",
    "    model.data.test_ds.write_predictions_to_file=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5c0fdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!head -n 10 results/medalQA_result_lora_tuning__test_medal_inputs_preds_labels.jsonl | jq ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb362908-4345-4f62-a60d-0a075005259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def compute_f1(pred: str, ref: str) -> float:\n",
    "    pred_tokens = pred.lower().split()\n",
    "    ref_tokens = ref.lower().split()\n",
    "    common = set(pred_tokens) & set(ref_tokens)\n",
    "    num_common = sum(min(pred_tokens.count(tok), ref_tokens.count(tok)) for tok in common)\n",
    "    if num_common == 0:\n",
    "        return 0.0\n",
    "    precision = num_common / len(pred_tokens)\n",
    "    recall = num_common / len(ref_tokens)\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# Evaluate results from combined JSONL\n",
    "file_path = 'results/medalQA_result_lora_tuning__test_medal_inputs_preds_labels.jsonl'\n",
    "\n",
    "exacts = []\n",
    "f1s = []\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        pred = obj.get('pred', '').strip()\n",
    "        label = obj.get('label', '').strip()\n",
    "        exacts.append(float(pred == label))\n",
    "        f1s.append(compute_f1(pred, label))\n",
    "\n",
    "# Aggregate and display\n",
    "avg_em = sum(exacts) / len(exacts)\n",
    "avg_f1 = sum(f1s) / len(f1s)\n",
    "print(f\"Average Exact Match (EM): {avg_em:.3f}\")\n",
    "print(f\"Average F1 Score: {avg_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6453e9-cfe6-4171-849f-b1a8cc860791",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash -s \"$token\"\n",
    "\n",
    "# OPTIONAL: Assess performance of the original model\n",
    "\n",
    "# Log in to HuggingFace to get AutoTokenizer with pretrained_model_name\n",
    "HF_TOKEN=\"$1\"\n",
    "huggingface-cli login --token \"$HF_TOKEN\"\n",
    "\n",
    "PRECISION=bf16\n",
    "MODEL=\"models/llama-3.1-instruct/8B/nemo/$PRECISION/Llama-3_1-Instruct-8B.nemo\"\n",
    "TEST_DS=\"[datasets/medal_test.jsonl]\"\n",
    "TEST_NAMES=\"[medal]\"\n",
    "SCHEME=\"lora\"\n",
    "GPUS=4 # Adjust if necessary\n",
    "TP_SIZE=1\n",
    "PP_SIZE=1\n",
    "\n",
    "# The generation run will save the generated outputs over the test dataset in a file prefixed like so\n",
    "OUTPUT_PREFIX=\"results/medalQA_result_no_tuning_\"\n",
    "\n",
    "export PYTHONWARNINGS=\"ignore\"\n",
    "export TOKENIZERS_PARALLELISM=true\n",
    "\n",
    "torchrun --nproc_per_node=${GPUS} \\\n",
    "/opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py \\\n",
    "    model.restore_from_path=${MODEL} \\\n",
    "    trainer.devices=${GPUS} \\\n",
    "    trainer.num_nodes=1 \\\n",
    "    trainer.precision=${PRECISION} \\\n",
    "    model.megatron_amp_O2=True \\\n",
    "    model.global_batch_size=16 \\\n",
    "    model.micro_batch_size=4 \\\n",
    "    model.data.test_ds.file_names=${TEST_DS} \\\n",
    "    model.data.test_ds.names=${TEST_NAMES} \\\n",
    "    model.data.test_ds.global_batch_size=16 \\\n",
    "    model.data.test_ds.tokens_to_generate=128 \\\n",
    "    model.tensor_model_parallel_size=${TP_SIZE} \\\n",
    "    model.pipeline_model_parallel_size=${PP_SIZE} \\\n",
    "    inference.greedy=True \\\n",
    "    model.data.test_ds.output_file_path_prefix=${OUTPUT_PREFIX} \\\n",
    "    model.data.test_ds.write_predictions_to_file=True \\\n",
    "    model.data.test_ds.label_key='output' \\\n",
    "    model.data.test_ds.add_eos=True \\\n",
    "    model.data.test_ds.add_sep=False \\\n",
    "    model.data.test_ds.add_bos=False \\\n",
    "    model.data.test_ds.truncation_field=\"input\" \\\n",
    "    model.data.test_ds.prompt_template=\"\\{input\\} \\{output\\}\" \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d97d301-85ff-4e73-93a1-e74d230432d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 10 results/medalQA_result_no_tuning__test_medal_inputs_preds_labels.jsonl | jq ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c66a9c4-2705-4996-9cd8-6658b9a65e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def compute_f1(pred: str, ref: str) -> float:\n",
    "    pred_tokens = pred.lower().split()\n",
    "    ref_tokens = ref.lower().split()\n",
    "    common = set(pred_tokens) & set(ref_tokens)\n",
    "    num_common = sum(min(pred_tokens.count(tok), ref_tokens.count(tok)) for tok in common)\n",
    "    if num_common == 0:\n",
    "        return 0.0\n",
    "    precision = num_common / len(pred_tokens)\n",
    "    recall = num_common / len(ref_tokens)\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# Evaluate results from combined JSONL\n",
    "file_path = 'results/medalQA_result_no_tuning__test_medal_inputs_preds_labels.jsonl'\n",
    "\n",
    "exacts = []\n",
    "f1s = []\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        pred = obj.get('pred', '').strip()\n",
    "        label = obj.get('label', '').strip()\n",
    "        exacts.append(float(pred == label))\n",
    "        f1s.append(compute_f1(pred, label))\n",
    "\n",
    "# Aggregate and display\n",
    "avg_em = sum(exacts) / len(exacts)\n",
    "avg_f1 = sum(f1s) / len(f1s)\n",
    "print(f\"Average Exact Match (EM): {avg_em:.3f}\")\n",
    "print(f\"Average F1 Score: {avg_f1:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

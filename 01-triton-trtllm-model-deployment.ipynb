{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "323f65fd-c390-4b61-804f-e5cfcb70a7ef",
   "metadata": {},
   "source": [
    "<center>\n",
    "  <a href=\"https://escience.sdu.dk/index.php/ucloud/\">\n",
    "    <img src=\"https://escience.sdu.dk/wp-content/uploads/2020/03/logo_esc.svg\" width=\"400\" height=\"186\" />\n",
    "  </a>\n",
    "</center>\n",
    "<br>\n",
    "<p style=\"font-size: 1.2em;\">\n",
    "  This notebook was tested using <strong>Triton Inference Server (TRT-LLM) v25.02</strong> and machine type <code>uc1-l40-4</code> on UCloud.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d7e1b0-6ab5-4f7c-bd2f-b51c916d6a90",
   "metadata": {},
   "source": [
    "# 01 - Deploying a Triton Inference Server for LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bb3216-f529-42d7-a4b5-700a8ffc0390",
   "metadata": {},
   "source": [
    "## üöÄ Introduction\n",
    "\n",
    "In this end‚Äëto‚Äëend tutorial, we‚Äôll take a pre-trained LLM and:\n",
    "1. Authenticate and download weights from Hugging Face.\n",
    "2. Convert the model into a **TensorRT-LLM** engine for high‚Äëperformance inference.\n",
    "3. Test the optimized engine locally.\n",
    "4. Package and deploy the engine on NVIDIA Triton Inference Server with inflight batching.\n",
    "5. Send sample requests to Triton and validate responses.\n",
    "6. Profile performance using Triton‚Äôs `genai-perf` tool.\n",
    "\n",
    "> üõ†Ô∏è **Important Environment Note:**\n",
    "> This notebook is designed to run on **UCloud**, using the **NVIDIA Triton Inference Server (TRT-LLM) app, version `v25.02`**.\n",
    "> If you encounter unexpected errors, **double-check you are using the correct app version**, and that your session includes **GPU resources**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0462d8-0a60-4c48-b8dd-06ae560be1dd",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Step 1: Hugging Face Authentication\n",
    "\n",
    "The following code creates a secure input widget for your Hugging Face token, which is required to authenticate and download the [Llama 3.1 Nemotron Nano 4B v1.1](https://huggingface.co/nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1) model from the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec322c9-9520-40bd-a75f-459ec0b6bc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import Password\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "pwd = Password(description=\"Hugging Face Token:\")\n",
    "display(pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcbe2c4-d9ce-4708-bbf4-479b56db40d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = pwd.value\n",
    "hf_model=\"nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1\"\n",
    "hf_model_path=\"models/llama-3.1-nemotron-nano/4B/hf\"\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=hf_model,\n",
    "    local_dir=hf_model_path,\n",
    "    token=token\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb284d44-c990-4b16-b650-acb3e3be370d",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Step 2: Convert the Model Checkpoint to TensorRT-LLM Format\n",
    "\n",
    "The following Python script sets up the required directories and executes the conversion of the model checkpoint from Hugging Face to a **TensorRT-ready** format for optimized performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ced79cc-20cb-43af-adec-fabd75697062",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "HF_MODEL=\"models/llama-3.1-nemotron-nano/4B/hf\"\n",
    "\n",
    "du -sh \"$HF_MODEL\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c8087d-4314-48cc-b773-291cfd247ecd",
   "metadata": {},
   "source": [
    "The table below compares two deployment strategies for a 4B parameter model on dual NVIDIA L40 GPUs‚Äîusing one GPU per replica (TP = 1, PP = 1) versus sharding a single replica across both GPUs (TP = 2)‚Äîhighlighting the trade-offs in interconnect usage, TensorRT-LLM support, and tuning complexity.\n",
    "\n",
    "| Criterion                            | TP = 1, PP = 1 (one GPU per replica)                                                                                        | TP = 2 (shard across both GPUs)                                                                                                       |\n",
    "| ------------------------------------ | --------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **L40 interconnect**                 | No cross-GPU traffic. The card has **no NVLink; only PCIe Gen4 √ó16 (64 GB/s)**, so you stay off the slow bus. ([NVIDIA][1]) | Every attention & FFN layer does an all-reduce over PCIe, erasing any speed-up.                                                       |\n",
    "| **Software support in TensorRT-LLM** | Fully supported; Triton can run many single-GPU engines via *orchestrator* or *leader* mode. ([NVIDIA Docs][2])             | Works, but gives no concurrency benefit and complicates launch scripts.                                                               |\n",
    "| **Tuning knobs**                     | Only one engine build: `--tp_size 1`.  Simpler GPU placement and easier scaling tests.                                      | Must align hidden-size and #heads to TP; increased build time; more configs to tune.                                                  |\n",
    "\n",
    "[1]: https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/support-guide/NVIDIA-L40-Datasheet-January-2023.pdf \"NVIDIA L40\"\n",
    "[2]: https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/tensorrtllm_backend/docs/llama_multi_instance.html \"Running Multiple Instances of the LLaMa Model ‚Äî NVIDIA Triton Inference Server\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fc9a83-3659-4ce9-be7a-c077098d1511",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "HF_MODEL=\"models/llama-3.1-nemotron-nano/4B/hf\"\n",
    "TRT_CKPT=\"models/llama-3.1-nemotron-nano/4B/trt_ckpt/tp1\"\n",
    "mkdir -p \"$TRT_CKPT\"\n",
    "\n",
    "python -W ignore ~/llama/convert_checkpoint.py \\\n",
    "      --model_dir \"$HF_MODEL\" \\\n",
    "      --output_dir \"$TRT_CKPT\" \\\n",
    "      --dtype bfloat16 \\\n",
    "      --tp_size 1 \\\n",
    "      --load_by_shard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5411031-5aa0-4e27-935b-c847fb345a44",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Step 3: Build TensorRT-LLM Engine\n",
    "\n",
    "The following script constructs the **TensorRT-LLM** engine from the previously converted model checkpoint. This optimization enhances the model's inference performance by leveraging TensorRT's efficient execution capabilities.\n",
    "\n",
    "**Checkpoint vs. Engine Directory Contents**\n",
    "\n",
    "| Directory                                        | Files                                | Purpose                                                                                                                                                                                                                                                                   |\n",
    "| ------------------------------------------------ | ------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `models/llama-3.1-nemotron-nano/4B/trt_ckpt/tp1` | `config.json`<br>`rank0.safetensors` | **Intermediate checkpoint.**<br>‚Ä¢ `rank0.safetensors` contains all BF16 weights for the single tensor-parallel rank.<br>‚Ä¢ `config.json` records hidden size, number of layers/heads, tokenizer info, and that `tp_size = 1` (so the builder knows no sharding is needed). |\n",
    "| `models/llama-3.1-nemotron-nano/4B/trt_llm/tp1`  | `config.json`<br>`rank0.engine`      | **Final TensorRT-LLM engine.**<br>‚Ä¢ `rank0.engine` is the serialized, fused CUDA graph for the whole model.<br>‚Ä¢ Includes the GEMM/GELU/rotary-positional kernels and paged-KV-cache support you enabled (`--paged_kv_cache enable`).                                     |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b55df1-420c-4530-bab0-b010efc6c9fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "TRT_CKPT=\"models/llama-3.1-nemotron-nano/4B/trt_ckpt/tp1\"\n",
    "TRT_ENGINE=\"models/llama-3.1-nemotron-nano/4B/trt_llm/tp1\"\n",
    "\n",
    "trtllm-build --checkpoint_dir \"$TRT_CKPT\" \\\n",
    "      --output_dir \"$TRT_ENGINE\" \\\n",
    "      --gemm_plugin auto \\\n",
    "      --max_num_tokens 16384 \\\n",
    "      --max_input_len 4096  \\\n",
    "      --max_seq_len 8192 \\\n",
    "      --kv_cache_type paged \\\n",
    "      --workers 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0734ff3-1f3e-49a6-a69b-534bf04ef9ae",
   "metadata": {},
   "source": [
    "**Local Testing of TensorRT-Optimized model**\n",
    "\n",
    "The following Python script performs a local test of the optimized model checkpoint. It sets the necessary environment variables and uses a sample prompt to evaluate the model's inference performance.\n",
    "\n",
    "If you get an error in the cell below, update the Transformer library:\n",
    "```bash\n",
    "pip install -U Transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01eb42e3-6dc6-4692-a11e-d887dd8321ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "HF_MODEL=\"models/llama-3.1-nemotron-nano/4B/hf\"\n",
    "TRT_ENGINE=\"models/llama-3.1-nemotron-nano/4B/trt_llm/tp1\"\n",
    "\n",
    "PROMPT='{\"role\":\"system\",\"content\":\"detailed thinking off\"}\n",
    "{\"role\":\"user\",\"content\":\"What are the typical symptoms of an infected appendix?\"}\n",
    "{\"role\":\"assistant\",\"content\":\"<think>\\n</think>\"}'\n",
    "\n",
    "python ~/run.py \\\n",
    "    --engine_dir     \"$TRT_ENGINE\" \\\n",
    "    --tokenizer_dir  \"$HF_MODEL\" \\\n",
    "    --input_text     \"$PROMPT\" \\\n",
    "    --max_output_len 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ba3398-107e-4856-afc2-113070a1e808",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Step 4: Deploying Triton with Inflight Batching\n",
    "\n",
    "The following scripts set up and configure Triton Inference Server for the model engine using [**inflight batching**](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/batcher.html) and [**orchestrator mode**](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/tensorrtllm_backend/docs/llama_multi_instance.html#orchestrator-mode). This deployment optimizes inference performance by managing batch sizes and instance counts effectively.\n",
    "\n",
    "Using the C++ TensorRT-LLM backend with the executor API.\n",
    "\n",
    "| Model            | Description                                                                                           |\n",
    "|------------------|-------------------------------------------------------------------------------------------------------|\n",
    "| **ensemble**     | This model is used to chain the preprocessing, tensorrt_llm, and postprocessing models together.      |\n",
    "| **preprocessing**| This model is used for tokenizing, meaning the conversion from prompts (string) to input_ids (ints).  |\n",
    "| **tensorrt_llm** | This model is a wrapper of your TensorRT-LLM model and is used for inferencing.                       |\n",
    "| **postprocessing**| This model is used for de-tokenizing, meaning the conversion from output_ids (ints) to outputs (string). |\n",
    "| **tensorrt_llm_bls** | This model can also be used to chain the preprocessing, tensorrt_llm, and postprocessing models together. |\n",
    "\n",
    "**Memory math: 4 B model on a 48 GB L40**\n",
    "\n",
    "| Component (per replica)                   | Footprint (BF16) | Notes                                           |\n",
    "| ----------------------------------------- | ---------------- | ----------------------------------------------- |\n",
    "| Weights + constant buffers                | **‚âà 8.0 GB**     | 4B params √ó 2 bytes √ó (1 + small overhead)      |\n",
    "| CUDA Graph & scratch                      | ‚âà 1 GB           | TensorRT kernels, persistent workspace          |\n",
    "| **KV cache** (*paged*, 16384 tokens cap)  | **‚â§ 4.2 GB**     | 32 layers √ó 4096 dim √ó 2 bytes √ó 16384 tokens   |\n",
    "| **Total per replica**                     | **‚âà 13 GB**      | round-up for safety                             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb947618-8ac0-4f98-b2ff-e2400cf28120",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "TRTITON_REPO=\"models/llama-3.1-nemotron-nano/4B/triton\"\n",
    "mkdir -p \"$TRTITON_REPO\"\n",
    "\n",
    "cp -r ~/all_models/inflight_batcher_llm/* \"$TRTITON_REPO\"\n",
    "\n",
    "ls \"$TRTITON_REPO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "897a7902-effd-4235-a1a8-0dec875b9e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# preprocessing\n",
    "ENGINE_DIR=\"models/llama-3.1-nemotron-nano/4B/trt_llm/tp1\"\n",
    "TOKENIZER_DIR=\"models/llama-3.1-nemotron-nano/4B/hf\"\n",
    "MODEL_FOLDER=\"models/llama-3.1-nemotron-nano/4B/triton\"\n",
    "TRITON_MAX_BATCH_SIZE=16 # reduce this value if you encounter OOM cuda errors\n",
    "\n",
    "# How many parallel workers Triton should spin up for this model?\n",
    "# For preprocessing / postprocessing (CPU) INSTANCE_COUNT controls how many tokenisers / detokenisers you can run in parallel\n",
    "# raising it improves throughput until CPU cores or the GPU generation stage saturate\n",
    "INSTANCE_COUNT=16 # Requests that can be tokenised and detokenised in parallel\n",
    "MAX_QUEUE_DELAY_MS=1000 # Helps collect more requests into batches at very low overhead, or set to zero if you want lowest possible latency\n",
    "MAX_QUEUE_SIZE=512 # Allow some queuing under bursty load, or set to zero for pure online inferencing\n",
    "FILL_TEMPLATE_SCRIPT=\"$HOME/tools/fill_template.py\"\n",
    "LOGITS_DATA_TYPE=\"TYPE_FP32\"\n",
    "DECOUPLED_MODE=false # No decoupled streaming, matches inflight batching\n",
    "\n",
    "python ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/ensemble/config.pbtxt triton_max_batch_size:${TRITON_MAX_BATCH_SIZE},logits_datatype:${LOGITS_DATA_TYPE}\n",
    "python ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/preprocessing/config.pbtxt tokenizer_dir:${TOKENIZER_DIR},triton_max_batch_size:${TRITON_MAX_BATCH_SIZE},preprocessing_instance_count:${INSTANCE_COUNT}\n",
    "python ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/tensorrt_llm/config.pbtxt triton_backend:tensorrtllm,triton_max_batch_size:${TRITON_MAX_BATCH_SIZE},decoupled_mode:${DECOUPLED_MODE},engine_dir:${ENGINE_DIR},max_queue_delay_microseconds:${MAX_QUEUE_DELAY_MS},batching_strategy:inflight_fused_batching,max_queue_size:${MAX_QUEUE_SIZE},logits_datatype:${LOGITS_DATA_TYPE},encoder_input_features_data_type:TYPE_FP16 \n",
    "python ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/postprocessing/config.pbtxt tokenizer_dir:${TOKENIZER_DIR},triton_max_batch_size:${TRITON_MAX_BATCH_SIZE},postprocessing_instance_count:${INSTANCE_COUNT},max_queue_size:${MAX_QUEUE_SIZE}\n",
    "python ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/tensorrt_llm_bls/config.pbtxt triton_max_batch_size:${TRITON_MAX_BATCH_SIZE},decoupled_mode:${DECOUPLED_MODE},bls_instance_count:${INSTANCE_COUNT},logits_datatype:${LOGITS_DATA_TYPE}\n",
    "\n",
    "\n",
    "# For starting one model replica per available GPU\n",
    "GPU_NUM=4\n",
    "GPU_IDS=\"0;1;2;3\"\n",
    "    \n",
    "python ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/tensorrt_llm/config.pbtxt gpu_device_ids:${GPU_IDS}\n",
    "\n",
    "sed -i '/instance_group *\\[/,/\\]/d' \"$MODEL_FOLDER/tensorrt_llm/config.pbtxt\"\n",
    "\n",
    "cat <<EOF >> \"$MODEL_FOLDER/tensorrt_llm/config.pbtxt\"\n",
    "instance_group [\n",
    "  {\n",
    "    count: ${GPU_NUM}\n",
    "    kind: KIND_CPU\n",
    "  }\n",
    "]\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bb0716-5424-41e4-a5da-78d043fc7e72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# The following command starts an OpenAI-Compatible Frontend for Triton Inference Server\n",
    "# This is used to run predictions in Label Studio\n",
    "\n",
    "MODEL_FOLDER=\"models/llama-3.1-nemotron-nano/4B/triton\"\n",
    "TOKENIZER_DIR=\"models/llama-3.1-nemotron-nano/4B/hf\"\n",
    "\n",
    "stop_tritonserver\n",
    "\n",
    "# Run multiple istances of the model (one replica per GPU):\n",
    "# start in multi-process (MPI) orchestrator mode\n",
    "export TRTLLM_ORCHESTRATOR=1 \n",
    "\n",
    "nohup python3 -W ignore /opt/tritonserver/python/openai/openai_frontend/main.py --model-repository $MODEL_FOLDER --backend tensorrtllm --tokenizer $TOKENIZER_DIR --openai-port 9000 --enable-kserve-frontends &> /work/triton-server-log.txt &\n",
    "# nohup python3 -W ignore /opt/tritonserver/python/openai/openai_frontend/main.py --model-repository $MODEL_FOLDER --backend tensorrtllm --tokenizer $TOKENIZER_DIR --openai-port 9000 &> /work/triton-server-log.txt &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd45e251-b116-4b5e-80ba-4a183eb98914",
   "metadata": {},
   "source": [
    "The following Bash commands verify that the Triton server and the deployed model are running correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac0b462-23f2-4959-9c6d-b518b89c6138",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "LOG_FILE=\"/work/triton-server-log.txt\"\n",
    "\n",
    "# Function to wait for Triton to start by monitoring the log file\n",
    "wait_for_triton_start() {\n",
    "    echo \"Waiting for Triton Inference Server to start...\"\n",
    "    while true; do\n",
    "        # Check for all required startup messages\n",
    "        if grep -q 'Uvicorn running' \"$LOG_FILE\"; then\n",
    "                echo \"‚úÖ Uvicorn has started successfully.\"\n",
    "                break\n",
    "                \n",
    "        else\n",
    "            echo \"‚ùå Uvicorn has NOT started.. Retrying in 5 seconds...\"\n",
    "            sleep 5   \n",
    "        fi\n",
    "    done\n",
    "}\n",
    "\n",
    "# Wait for Triton to start\n",
    "wait_for_triton_start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bc2515-138d-4b6c-894d-5a2043594bc2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "To test the model, run the following code in a terminal window:\n",
    "```bash\n",
    "MODEL=ensemble\n",
    "\n",
    "TEXT=\"An appendectomy is a surgical procedure to remove the appendix when it's infected. Symptoms typically include abdominal pain starting around the navel and shifting to the lower right abdomen, nausea, vomiting, and fever. Diagnosis is often made based on symptoms, physical examination, and imaging like ultrasound or CT scan. The procedure can be performed using open surgery or minimally invasive laparoscopic techniques. Recovery usually takes a few weeks, with minimal complications.\"\n",
    "\n",
    "curl -s http://localhost:9000/v1/chat/completions \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "        \"model\": \"'\"${MODEL}\"'\",\n",
    "        \"messages\": [\n",
    "          {\"role\": \"system\", \"content\": \"detailed thinking off\\n\"},\n",
    "          {\"role\": \"assistant\", \"content\":\"<think>\\n</think>\\n\"},\n",
    "          {\"role\": \"user\",\n",
    "           \"content\": \"Given this medical text:\\n\\n'\"${TEXT}\"' \\n\\nGenerate direct, succinct, and unique medical questions covering symptoms, diagnosis, treatments, or patient management strategies. Output ONLY 5 questions clearly separated by new lines.\"\n",
    "           }\n",
    "        ],\n",
    "        \"temperature\": 0.2,\n",
    "        \"max_tokens\": 500\n",
    "}' | jq -r '.choices[0].message.content'\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501e523f-8fc2-4d59-a987-9ed0544e32a7",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Step 5: Performance Profiling with `genai-perf`\n",
    "\n",
    "To evaluate the performance of the deployed Triton Inference Server, execute the following Bash commands in a terminal session within Jupyter. This script uses [GenAI-Perf](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/perf_analyzer/genai-perf/README.html) to profile the model, generating performance metrics and visualizations.\n",
    "\n",
    "- Each request simulates:\n",
    "    - An **input prompt** of around **200 tokens** (¬±10 tokens random noise)\n",
    "    - And expects **200 output tokens** exactly.\n",
    "- **1000 total prompts** per test, but each test measures for **10 seconds**.\n",
    "- **Concurrency** will sweep from **10** to **500** (500 requests *in flight* at the same time).\n",
    "- The input and output sizes are **large** ‚Äî total per request is ~400 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a1db57-d2c4-4e27-99ec-742236f24593",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Run the following code in a terminal window for a better output format\n",
    "\n",
    "TOKENIZER_DIR=\"models/llama-3.1-nemotron-nano/4B/hf\"\n",
    "\n",
    "export INPUT_PROMPTS=1000\n",
    "export INPUT_SEQUENCE_LENGTH=200\n",
    "export INPUT_SEQUENCE_STD=10\n",
    "export OUTPUT_SEQUENCE_LENGTH=200\n",
    "export MODEL=ensemble\n",
    "\n",
    "# Running multiple GenAI-Perf calls (the first for warm-up)\n",
    "for concurrency in 1 1 2 4 8 16 32 48 64 128 256 512 1024; do\n",
    "    genai-perf profile -m $MODEL \\\n",
    "        --service-kind triton \\\n",
    "        --backend tensorrtllm \\\n",
    "        --num-prompts $INPUT_PROMPTS \\\n",
    "        --random-seed 1234 \\\n",
    "        --synthetic-input-tokens-mean $INPUT_SEQUENCE_LENGTH \\\n",
    "        --synthetic-input-tokens-stddev $INPUT_SEQUENCE_STD \\\n",
    "        --output-tokens-mean $OUTPUT_SEQUENCE_LENGTH \\\n",
    "        --output-tokens-stddev 0 \\\n",
    "        --output-tokens-mean-deterministic \\\n",
    "        --tokenizer $TOKENIZER_DIR \\\n",
    "        --concurrency $concurrency \\\n",
    "        --measurement-interval 25000 \\\n",
    "        --profile-export-file model_profile_${concurrency}.json \\\n",
    "        --url \"localhost:8001\" \\\n",
    "        --generate-plots\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cdc0fb-697d-4853-8605-ebedf5829c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import json\n",
    "\n",
    "# Settings\n",
    "concurrency_levels = [1, 2, 4, 8, 16, 32, 48, 64, 128, 256, 512, 1024]  # Concurrency values you tested\n",
    "base_path = \"artifacts/ensemble-triton-tensorrtllm-concurrency{}\"\n",
    "\n",
    "# Storage for points\n",
    "x = []  # Time to first token (s)\n",
    "y = []  # Total system throughput (tokens/sec)\n",
    "labels = []  # Concurrency values\n",
    "\n",
    "# Read each JSON file\n",
    "for concurrency in concurrency_levels:\n",
    "    folder = base_path.format(concurrency)\n",
    "    json_path = os.path.join(folder, f\"model_profile_{concurrency}_genai_perf.json\")\n",
    "    \n",
    "    # Read JSON file\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Extract values\n",
    "    time_to_first_token_ms = data['time_to_first_token']['avg']  # milliseconds\n",
    "    output_token_throughput = data['output_token_throughput']['avg']  # tokens per second\n",
    "\n",
    "    # Append\n",
    "    x.append(time_to_first_token_ms / 1000)  # ms ‚Üí seconds\n",
    "    y.append(output_token_throughput)\n",
    "    labels.append(concurrency)\n",
    "\n",
    "\n",
    "# Now plot\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x, y=y,\n",
    "    mode='lines+markers+text',\n",
    "    text=labels,\n",
    "    textposition=\"middle center\",\n",
    "    line=dict(width=2),\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Time to first token (s)\",\n",
    "    yaxis_title=\"Throughput (tokens/s)\",\n",
    "    plot_bgcolor=\"rgba(240, 248, 255, 1)\",\n",
    "    font=dict(size=14),\n",
    "    width=1000,\n",
    "    height=800,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
